{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78e3ddfb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Weekly Community Detection using Louvains Algorithm\n",
    "\n",
    "#### Author: Boikanyo Radiokana\n",
    "#### Version: 1.1\n",
    "#### Description: Application of Communtiy detection algorithms to detect entitities who engage in commission abuse activities\n",
    "#### Dataset: Daily Transaction rollup\n",
    "#### Market: Tanzania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1ef7808",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade pip\n",
    "# !pip install pyathena\n",
    "# !pip install pandas\n",
    "# !pip install --upgrade botocore==1.27.12\n",
    "# !pip install awswrangler\n",
    "# !pip install scipy\n",
    "# !pip install pyvis\n",
    "# !pip install networkx\n",
    "# !pip install sklearn\n",
    "# !pip install python-louvain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff17ffa4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyathena import connect\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import numpy as np\n",
    "import awswrangler as wr\n",
    "import boto3\n",
    "from pyvis.network import Network\n",
    "import networkx as nx\n",
    "\n",
    "import pickle\n",
    "import bz2\n",
    "from time import strftime\n",
    "import logging\n",
    "import sys\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "from datetime import timedelta\n",
    "from community import community_louvain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc5a431",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c881b829",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_data(bucket: str, prefix: str, filetype: str):\n",
    "    #Get data from an S3 bucket >> read parquet files and combine into 1 df\n",
    "    conn = boto3.client('s3')\n",
    "    contents = conn.list_objects(Bucket=bucket, Prefix=prefix)['Contents']\n",
    "    filenames = []\n",
    "    for content in contents:\n",
    "        filenames.append(f\"s3://{bucket}/{content['Key']}\")\n",
    "    \n",
    "    subset_filenames = filenames[1:] #Change as per your requirement \n",
    "    \n",
    "    if filetype == 'CSV':\n",
    "        data = wr.s3.read_csv(path=subset_filenames)\n",
    "        \n",
    "    elif filetype == 'PARQUET':\n",
    "        data = wr.s3.read_parquet(path=subset_filenames)    \n",
    "    return data\n",
    "\n",
    "\n",
    "def fill_nulls(df:pd.DataFrame):\n",
    "\n",
    "    df.rename(columns={'PartyA':'identity_a', 'PartyB':'identity_b'}, inplace=True)\n",
    "    df.loc[(df.tt_description=='Cash In' ) & (df.identity_a.isnull()) ,'identity_a'] = df.loc[(df.tt_description=='Cash In' ) & (df.identity_a.isnull()) ,'identity_a'].fillna('Agent')\n",
    "    df.loc[(df.tt_description=='Cash In' ) & (df.identity_b.isnull()) ,'identity_b'] = df.loc[(df.tt_description=='Cash In' ) & (df.identity_b.isnull()) ,'identity_b'].fillna('Customer')\n",
    "\n",
    "    df.loc[(df.tt_description=='Cash Out' ) & (df.identity_a.isnull()) ,'identity_a'] = df.loc[(df.tt_description=='Cash Out' ) & (df.identity_a.isnull()) ,'identity_a'].fillna('Customer')\n",
    "    df.loc[(df.tt_description=='Cash Out' ) & (df.identity_b.isnull()) ,'identity_b'] = df.loc[(df.tt_description=='Cash Out' ) & (df.identity_b.isnull()) ,'identity_b'].fillna('Agent')\n",
    "\n",
    "    df.loc[(df.tt_description=='Organisation To Organisation Transfer' ) & (df.identity_a.isnull()) ,'identity_a'] = df.loc[(df.tt_description=='Organisation To Organisation Transfer' ) & (df.identity_a.isnull()) ,'identity_a'].fillna('Agent')\n",
    "    df.loc[(df.tt_description=='Organisation To Organisation Transfer' ) & (df.identity_b.isnull()) ,'identity_b'] = df.loc[(df.tt_description=='Organisation To Organisation Transfer' ) & (df.identity_b.isnull()) ,'identity_b'].fillna('Agent')\n",
    "\n",
    "\n",
    "    df.loc[(df.tt_description=='Merchant Cash Out' ) & (df.identity_a.isnull()) ,'identity_a'] = df.loc[(df.tt_description=='Merchant Cash Out' ) & (df.identity_a.isnull()) ,'identity_a'].fillna('Merchant')\n",
    "    df.loc[(df.tt_description=='Merchant Cash Out' ) & (df.identity_b.isnull()) ,'identity_b'] = df.loc[(df.tt_description=='Merchant Cash Out' ) & (df.identity_b.isnull()) ,'identity_b'].fillna('Agent')\n",
    "\n",
    "\n",
    "    df.loc[(df.tt_description=='Pay Bill OTC' ) & (df.identity_a.isnull()) ,'identity_a'] = df.loc[(df.tt_description=='Pay Bill OTC' ) & (df.identity_a.isnull()) ,'identity_a'].fillna('Agent')\n",
    "    df.loc[(df.tt_description=='Pay Bill OTC' ) & (df.identity_b.isnull()) ,'identity_b'] = df.loc[(df.tt_description=='Pay Bill OTC' ) & (df.identity_b.isnull()) ,'identity_b'].fillna('Business')\n",
    "\n",
    "    df.loc[df.identity_a.isnull() ,'identity_a'] = df.loc[df.identity_a.isnull() ,'identity_a'].fillna('Unknown')\n",
    "    df.loc[df.identity_b.isnull() ,'identity_b'] = df.loc[df.identity_b.isnull() ,'identity_b'].fillna('Unknown')\n",
    "    \n",
    "    df.drop(columns=['RT_ID#0','C_DATA_KEY','C_DATA_VALUE','rundate'],axis=1, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def fill_missing_values(df: pd.DataFrame):\n",
    "\n",
    "    df[['amount','mpesa_fee_revenue','mpesa_comm_expense','comm_revenue_a','comm_revenue_b','fee_expense_a','fee_revenue_a','fee_revenue_b','fee_expense_b','levy']] =(df.loc[:,['amount','mpesa_fee_revenue',\n",
    "    'mpesa_comm_expense','comm_revenue_a','comm_revenue_b','fee_expense_a','fee_revenue_a','fee_revenue_b','fee_expense_b','levy']].fillna(0))\n",
    "    \n",
    "    df[['id_a', 'it_a', 'at_a', 'id_b', 'it_b', 'at_b',  'tt_id', 'rt_id',]] =df.loc[:,['id_a', 'it_a', 'at_a', 'id_b', 'it_b', 'at_b',  'tt_id', 'rt_id']].astype('str')\n",
    "   \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def get_comm_transactions(df: pd.DataFrame):\n",
    "    #filter only on transactions that have commission greater than 0\n",
    "    df1 = df[(df['comm_revenue_a']>0) | (df['comm_revenue_b']>0)] \n",
    "    df2 = df1[df1['tt_id'] !=  6625 ]\n",
    "    df3 = df2[df2['tt_id'] != 6627 ]\n",
    "    df4 = df3[df3['tt_id'] != 6565 ]\n",
    "  \n",
    "    return df4\n",
    "\n",
    "def preprocessing(bucket: str, prefix: str, filetype: str):\n",
    "\n",
    "    transrollup = get_data(bucket, prefix,filetype)\n",
    "    transrollup1 = fill_missing_values(transrollup)\n",
    "    transrollup2=  fill_nulls(transrollup1)\n",
    "    transrollup3 = get_comm_transactions(transrollup2)\n",
    "\n",
    "    return transrollup3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e86e4d",
   "metadata": {},
   "source": [
    "# Graph Model and Savings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c30365e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_graph(df: pd.DataFrame ,graph_type: str , weight: float):    \n",
    "    #create a directional graph using the transaction rollup as an edge list\n",
    "    G = nx.DiGraph()    \n",
    "    if graph_type == 'weighted': \n",
    "        #load the edges\n",
    "        edge_list=list(df[['id_a','id_b',weight]].itertuples(index=False, name=None))\n",
    "        G.add_weighted_edges_from(edge_list)\n",
    "        \n",
    "    elif graph_type == 'not_weighted': \n",
    "        edge_list=list(df[['id_a','id_b']].itertuples(index=False, name=None))\n",
    "        G.add_edges_from(edge_list)      \n",
    "    else:\n",
    "        return \"Graph Not Specified!\"\n",
    "    return G\n",
    "\n",
    "\n",
    "# Model Pickling \n",
    "def _compress_file(model):\n",
    "    pickle_byte_obj = pickle.dumps(model)\n",
    "    compressed = bz2.compress(pickle_byte_obj)\n",
    "    logging.info(f'Compression Ratio achieved: {sys.getsizeof(compressed)/sys.getsizeof(model)}')\n",
    "    return compressed\n",
    "\n",
    "def save_model(s3_resource, model, model_name, model_prefix, usecase_bucket):\n",
    "    timestamp = str(strftime(\"%Y%m%d_%H%M%S\"))\n",
    "    model_name = f'{model_name}_{timestamp}'\n",
    "    filename = f'{model_prefix}{model_name}.pkl'\n",
    "    path = f\"s3://{usecase_bucket}/{filename}\"\n",
    "    compressed = _compress_file(model)\n",
    "    s3_resource.Object(usecase_bucket,filename).put(Body=compressed)\n",
    "    logging.info(f'Model has been saved: {filename}')\n",
    "    return filename\n",
    "    \n",
    "def _decompress_file(loaded_obj):\n",
    "    decompressed = bz2.decompress(loaded_obj)\n",
    "    loaded_model = pickle.loads(decompressed)\n",
    "    return loaded_model\n",
    "\n",
    "def load_model(s3_resource, usecase_bucket, filename):\n",
    "    loaded_obj = s3_resource.Object(usecase_bucket,filename).get()[\"Body\"].read()\n",
    "    loaded_model = _decompress_file(loaded_obj)\n",
    "    logging.info(f'Model loaded.')\n",
    "    return loaded_model\n",
    "\n",
    "\n",
    "def is_pickled_success(main_graph,pickled_G):\n",
    "   # Check if Model was pickled correctly \n",
    "    main_nodes = main_graph.number_of_nodes()\n",
    "    pickled_nodes = pickled_G.number_of_nodes()\n",
    "\n",
    "    if main_nodes == pickled_nodes:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "    \n",
    "def graph_model_saving(df: pd.DataFrame, foldername: str, bucket:str):\n",
    "    #create the main graph\n",
    "    main_graph = create_graph(df , 'weighted','amount')\n",
    "\n",
    "    # Save the main graph model\n",
    "\n",
    "    key= f'output_data/mpesa/tanz/test/tanzania_comm_abuse_output/graph_model/{foldername}/'\n",
    "    model_name = f'main_graph_{foldername}'\n",
    "\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    model_filename =save_model(s3_resource, main_graph, model_name,key, bucket)\n",
    "    pickled_G = load_model(s3_resource, bucket, model_filename)\n",
    "    isSavedSuccess = is_pickled_success(main_graph, pickled_G)\n",
    "        \n",
    "    if isSavedSuccess == True:\n",
    "        return main_graph\n",
    "\n",
    "    else:\n",
    "        print('Saving Failed')\n",
    "        return\n",
    "     \n",
    "    # Enter fail state - Stop the process\n",
    "    return main_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f059a916",
   "metadata": {},
   "source": [
    "# Louvain's Community Detection Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4899897",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def community_detection(main_graph):\n",
    "    #convert the main graph as an undirected graph\n",
    "    H = main_graph.to_undirected()\n",
    "    communities = community_louvain.best_partition(H)\n",
    "    #Get the group of communities\n",
    "    comm_dict_df = pd.DataFrame(communities.items(), columns=['id','comm_group'])\n",
    "    list_of_c = comm_dict_df.groupby(['comm_group']).count()\n",
    "    list_of_c = list_of_c.reset_index()\n",
    "    list_of_c.rename(columns={'id':'Count_of_nodes'},inplace=True)\n",
    "    sorted_list_of_comms = list_of_c.sort_values(by = ['Count_of_nodes'], ascending = False)\n",
    "    \n",
    "    return comm_dict_df,sorted_list_of_comms\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ca3f52",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de2210f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_community_transactions(comm_group: int ,comm_dict_df: pd.DataFrame,transrollup_x:pd.DataFrame ):\n",
    "    #get all then nodes that belong to the respective group\n",
    "    sub_comm_dict = comm_dict_df[comm_dict_df['comm_group']==comm_group]\n",
    "    sub_comm_nodes = sub_comm_dict.iloc[:,0]\n",
    "    #Extract only the nodes in the subgraph from the edges\n",
    "    sub_comm_edges = transrollup_x[transrollup_x['id_a'].isin(sub_comm_nodes) | transrollup_x['id_b'].isin(sub_comm_nodes)]\n",
    "    sub_comm_edges.loc[:,'comm_group'] = comm_group\n",
    "\n",
    "    return sub_comm_edges\n",
    "    \n",
    "    \n",
    "def get_unique_ids(sub_comm_edges: pd.DataFrame):\n",
    "    #extract the unique ids from the sub community transactions\n",
    "    id_a = sub_comm_edges[['id_a','identity_a']]\n",
    "    id_b = sub_comm_edges[['id_b','identity_b']]\n",
    "    id_a = id_a.rename(columns={'id_a':'id','identity_a':'id_type'})\n",
    "    id_b = id_b.rename(columns={'id_b':'id','identity_b':'id_type'})    \n",
    "    ids = pd.concat([id_a, id_b])\n",
    "    #get unique list of ids\n",
    "    ids = ids.drop_duplicates(subset=['id','id_type'])\n",
    "    #get count per identity type\n",
    "    identities = ids.id_type.value_counts().rename_axis('identity_type').reset_index(name='counts')\n",
    "    identities\n",
    "    \n",
    "    return ids, identities\n",
    "    # return ids\n",
    "\n",
    "def aggregate_types(df: pd.DataFrame , col: str, comm_group:int,  list_of_comms: pd.DataFrame):\n",
    "    \n",
    "    for j in range(len(df)):\n",
    "        type_ =  df.loc[j, col]\n",
    "        count = df.loc[j,'counts']\n",
    "        list_of_comms.loc[comm_group,type_] = count\n",
    "    \n",
    "    return list_of_comms\n",
    "\n",
    "def get_other_features(sub_comm_edges: pd.DataFrame, list_of_comms: pd.DataFrame,comm_group:int, ids):\n",
    "    \n",
    "    total_amount = sub_comm_edges.amount.sum()\n",
    "    total_mpesa_fee_revenue = sub_comm_edges.mpesa_fee_revenue.sum()\n",
    "    total_mpesa_comm_expense = sub_comm_edges.mpesa_comm_expense.sum()\n",
    "\n",
    "    list_of_comms.loc[comm_group,'total_amount'] = total_amount\n",
    "    list_of_comms.loc[comm_group,'mpesa_fee_revenue'] = total_mpesa_fee_revenue \n",
    "    list_of_comms.loc[comm_group,'mpesa_comm_expense'] =  total_mpesa_comm_expense\n",
    "    list_of_comms.loc[comm_group,'number_of_nodes']=  len(ids)\n",
    "    list_of_comms.loc[comm_group,'number_of_transactions']= len(sub_comm_edges)\n",
    "    \n",
    "    return list_of_comms\n",
    "    \n",
    "    \n",
    "def get_margin_features(community_features: pd.DataFrame):\n",
    "    #calculate the revenue margin for each community\n",
    "    community_features['ave_trans_amount'] =  community_features.loc[:,'total_amount']/community_features.loc[:,'number_of_transactions']\n",
    "    community_features['revenue_margin'] = community_features.loc[:,'mpesa_fee_revenue']-community_features.loc[:,'mpesa_comm_expense']\n",
    "    community_features1 = community_features.sort_values(by=['revenue_margin'], ascending=True)\n",
    "    community_features2 = community_features1.drop(['Count_of_nodes'], axis=1)\n",
    "    \n",
    "    return community_features2\n",
    "    \n",
    "def save_subcomm_data_s3(comm_group: int, sub_comm_edges: pd.DataFrame, bucket:str,foldername: str):\n",
    "    #save csv files of community groups to s3\n",
    "    sub_community_name = str(comm_group)\n",
    "    dest_dir = f\"output_data/mpesa/tanz/test/tanzania_comm_abuse_output/{foldername}/sub_comm_rollup{sub_community_name}.csv\"\n",
    "    write_path = f\"s3://{bucket}/{dest_dir}\"       \n",
    "    wr.s3.to_csv(df=sub_comm_edges,path=write_path,dataset=False, index=False)\n",
    "    \n",
    "# #get 4CIT format\n",
    "def save_dataframe(features_dataframe, bucket, prefix, name):\n",
    "    timestamp = str(strftime(\"%Y%m%d_%H%M%S\"))\n",
    "    filename = f'weekly_commission_abuse_v001_{name}_final_result_{timestamp}.csv'\n",
    "    wr.s3.to_csv(df=features_dataframe, path=f's3://{bucket}/{prefix}/{filename}', dataset=False,index=False)\n",
    "    logging.info(f'Dataframe saved: s3://{bucket}/{prefix}/{filename}')\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "def sub_feature_engineering(list_of_comms, transrollup_x, foldername,comm_dict_df, sub_comm_group):\n",
    "   \n",
    "    bucket='aws-glue-assets-461675780654-af-south-1'   \n",
    "    feature4 = pd.DataFrame(columns = ['comm_group', 'Customer', 'Agent', 'Merchant', 'total_amount',\n",
    "       'mpesa_fee_revenue', 'mpesa_comm_expense', 'number_of_nodes',\n",
    "       'number_of_transactions', 'Business','Super Agent','Unknown','ave_trans_amount', 'revenue_margin'])\n",
    "    \n",
    "    for comm_group in range(len(list_of_comms)):\n",
    "\n",
    "        sub_comm_edges = get_community_transactions(comm_group, comm_dict_df,transrollup_x)\n",
    "        sub_comm_edges['comm_group'] = sub_comm_edges.apply(lambda row: f'{int(sub_comm_group)}_{int(row.comm_group)}', axis=1)\n",
    "        \n",
    "        total_mpesa_fee_revenue = sub_comm_edges.mpesa_fee_revenue.sum()\n",
    "        total_mpesa_comm_expense = sub_comm_edges.mpesa_comm_expense.sum()\n",
    "        margin = total_mpesa_fee_revenue - total_mpesa_comm_expense\n",
    "        comply = 0\n",
    "     \n",
    "        if (margin < 0):\n",
    "        \n",
    "            comply = 1\n",
    "            ids, identities = get_unique_ids(sub_comm_edges)\n",
    "\n",
    "            #Feature 1 - Count of entity descriptions\n",
    "            feature1 = aggregate_types(identities,'identity_type', comm_group, list_of_comms)        \n",
    "\n",
    "            #Features 2 - Other\n",
    "            feature2=  get_other_features(sub_comm_edges, feature1, comm_group,ids) \n",
    "            feature3 = feature2.fillna(0)\n",
    "\n",
    "            save_subcomm_data_s3(comm_group, sub_comm_edges,bucket,foldername )\n",
    "\n",
    "    #Features 4 - Revenue Margin\n",
    "    if  (comply == 1) :\n",
    "        feature4 =  get_margin_features(feature3)\n",
    "\n",
    "    return feature4\n",
    "\n",
    "    \n",
    "def feature_engineering(list_of_comms, transrollup_x, foldername,comm_dict_df):\n",
    "   \n",
    "    bucket='aws-glue-assets-461675780654-af-south-1'   \n",
    "    for comm_group in range(len(list_of_comms)):\n",
    "        \n",
    "        # print(comm_group)\n",
    "        sub_comm_edges = get_community_transactions(comm_group, comm_dict_df,transrollup_x)\n",
    "        \n",
    "    \n",
    "        if (sub_comm_edges.shape[0] >10):\n",
    "            \n",
    "            ids, identities = get_unique_ids(sub_comm_edges)\n",
    "\n",
    "            #Feature 1 - Count of entity descriptions\n",
    "            feature1 = aggregate_types(identities,'identity_type', comm_group, list_of_comms)        \n",
    "\n",
    "            #Features 2 - Other\n",
    "            feature2=  get_other_features(sub_comm_edges, feature1, comm_group,ids) \n",
    "            feature3 = feature2.fillna(0)\n",
    "\n",
    "            save_subcomm_data_s3(comm_group, sub_comm_edges,bucket,foldername )\n",
    "\n",
    "    #Features 4 - Revenue Margin\n",
    "\n",
    "    feature4 =  get_margin_features(feature3)\n",
    "        # return\n",
    "\n",
    "    return feature4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "350e9b5c-60b3-40bc-afcb-1a0eda9550a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_file_name():\n",
    "    \n",
    "    end_dt = (datetime.today().replace(hour=23, minute=59, second=59, microsecond=59)) - timedelta(days=1)\n",
    "   \n",
    "    #Start Date (First day of the past 7days) not including today\n",
    "    start_dt = end_dt - timedelta(days=6)\n",
    "    start_dt = start_dt.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "    \n",
    "    #get the day, month and year to create the week range for the file name\n",
    "    earliest_day = str(start_dt.day)\n",
    "    earliest_month = str(start_dt.month)\n",
    "    earliest_year = str(start_dt.year)\n",
    "\n",
    "    lastest_day = str(end_dt.day)\n",
    "    lastest_month = str(end_dt.month)\n",
    "    lastest_year =str(end_dt.year)\n",
    "    \n",
    "    #padding\n",
    "    if len(earliest_day) == 1:\n",
    "        earliest_day = earliest_day.rjust(2, '0')\n",
    "        \n",
    "    if len(earliest_month) == 1:\n",
    "        earliest_month = earliest_month.rjust(2, '0')\n",
    "    \n",
    "    if len(lastest_day) == 1:\n",
    "        lastest_day = lastest_day.rjust(2, '0')\n",
    "        \n",
    "    if len(lastest_month) == 1:\n",
    "         lastest_month = lastest_month.rjust(2, '0')\n",
    "    \n",
    "    week_range=\"{}_{}_{}-{}_{}_{}\".format(earliest_day,earliest_month,earliest_year,lastest_day,lastest_month,lastest_year )\n",
    "    \n",
    "    return week_range"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be5beb0",
   "metadata": {},
   "source": [
    "# Graph Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35c62348",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pyvisgraph(df, filename):\n",
    "    \n",
    "    # Graph layout\n",
    "    pg_netw = Network(height='1000px', width='100%',directed=True, bgcolor='black', font_color='white')\n",
    "    colour_dict = {'Super Agent':'blue',\n",
    "                   'Customer':'red',\n",
    "                   'Money Provider':'purple',\n",
    "                   'Agent':'green',\n",
    "                   'Super Dealer':'yellow',\n",
    "                   'Bank':'pink',\n",
    "                   'Other':'pink',\n",
    "                   'Merchant':'white', \n",
    "                   'Walker':'brown', \n",
    "                   'Utility':'violet',\n",
    "                   'Corporate':'violet',\n",
    "                   'Unkown':'orange',\n",
    "                  'Open API':'violet', \n",
    "                   'Business':'violet',\n",
    "                   'Vodacom Head Office':'violet',\n",
    "                   'Vodacom Store':'violet'\n",
    "                  }\n",
    "  \n",
    "    # set the physics layout of the network\n",
    "    pg_netw.barnes_hut()\n",
    "\n",
    "    sources = df['id_a'].astype(str)\n",
    "    targets =df['id_b'].astype(str)\n",
    "    weights = df['amount']\n",
    "    \n",
    "    name = df['trans_index']\n",
    "    src_cat = df['identity_a']\n",
    "    dst_cat =df['identity_b'] \n",
    "    \n",
    "    src_col = src_cat.replace(colour_dict)\n",
    "    dst_col = dst_cat.replace(colour_dict)\n",
    "\n",
    "    edge_data = zip(sources, targets, weights,src_cat, dst_cat, src_col, dst_col,name)\n",
    "\n",
    "    for e in edge_data:\n",
    "        src = e[0]\n",
    "        dst = e[1]\n",
    "        w = e[2]\n",
    "\n",
    "        pg_netw.add_node(src, src, title=e[3]+'_'+ str(e[0]),color=e[5])\n",
    "        pg_netw.add_node(dst, dst, title=e[4]+'_'+str(e[1]), color=e[6])\n",
    "        pg_netw.add_edge(src, dst, label=e[7] , value=w)\n",
    "\n",
    "    neighbor_map = pg_netw.get_adj_list()\n",
    "   \n",
    "    # return pg_netw.show(filename+'.html')\n",
    "    return pg_netw.generate_html(notebook=False)\n",
    "\n",
    "\n",
    "def write_graphs_to_s3 (neg_comm2, foldername, group_type):\n",
    "\n",
    "    for i in neg_comm2['comm_group']:\n",
    "  \n",
    "        group = str(i)\n",
    "        #get the index\n",
    "        filename = 'sub_graph_' + group\n",
    "\n",
    "        if group_type == 'main':\n",
    "            prefix = f's3://aws-glue-assets-461675780654-af-south-1/output_data/mpesa/tanz/test/tanzania_comm_abuse_output/groups/{foldername}/sub_comm_rollup{group}.csv'\n",
    "      \n",
    "        \n",
    "        elif group_type == 'sub':\n",
    "            x = group.split('_')\n",
    "            sub = x[0]\n",
    "            grp = x[1]\n",
    "            prefix = f's3://aws-glue-assets-461675780654-af-south-1/output_data/mpesa/tanz/test/tanzania_comm_abuse_output/groups/subgroups/{foldername}/{foldername}_group_{sub}/sub_comm_rollup{grp}.csv'\n",
    "            \n",
    "        dfx = wr.s3.read_csv(prefix)\n",
    "\n",
    "        #Get graph networkk - transaction rollup\n",
    "        graph_text =  pyvisgraph(dfx, filename)  \n",
    "\n",
    "\n",
    "        path = f's3://aws-glue-assets-461675780654-af-south-1/output_data/mpesa/tanz/test/tanzania_comm_abuse_output/graph_visuals/{foldername}/sub_graph_{group}.html'\n",
    "        \n",
    "        temp = 'weekly_temp_graph/'+filename+'.html'\n",
    "        file_ = open(temp, \"w\")\n",
    "        file_.write(graph_text)\n",
    "        file_.close()\n",
    "\n",
    "        wr.s3.upload(local_file=temp, path=path)\n",
    "        \n",
    "        if os.path.exists(temp):\n",
    "            os.remove(temp)\n",
    "        else:\n",
    "            print(\"Cannot delete the file as it doesn't exists\")\n",
    "  \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fae4c21-3c0b-4e6d-b976-cd79be41b279",
   "metadata": {},
   "source": [
    "## First cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7dafaa1-65ac-4d82-8210-7eed2f540f6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18 µs, sys: 18 µs, total: 36 µs\n",
      "Wall time: 44.3 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # To ignore settingwithcopywarning\n",
    "\n",
    "def first_cycle(bucket,prefix,filetype,foldername):\n",
    "    #preprocessing\n",
    "    \n",
    "    data_ = preprocessing(bucket,prefix,filetype)    #show output\n",
    "    \n",
    "    #graph model & community detection\n",
    "  \n",
    "    main_graph = graph_model_saving(data_, foldername, bucket)  #artefact\n",
    "    comm_dict, communities =  community_detection(main_graph)  #show outputs\n",
    "    \n",
    "    #feature extraction\n",
    "    groups_dest = 'groups/'+foldername\n",
    "    community_features= feature_engineering(communities, data_, groups_dest,comm_dict)  #show output #includes group artefact\n",
    "    ## remove unsaved communities\n",
    "    community_features1 = community_features[(community_features['number_of_transactions']!= 0) ]\n",
    "    features_dest = f'output_data/mpesa/tanz/test/tanzania_comm_abuse_output/features/{foldername}'\n",
    "    save_dataframe(community_features1, bucket, features_dest, 'comm_feat') #artefact\n",
    "    \n",
    "    #filter on large positive communities\n",
    "    comm_of_comm =community_features1[(community_features1['revenue_margin'] >0) & (community_features1['number_of_transactions'] >10000) ] \n",
    "    \n",
    "    return comm_of_comm,community_features1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9874b8a0-427e-4c74-aa40-cfb94f4fa448",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14 µs, sys: 0 ns, total: 14 µs\n",
      "Wall time: 23.4 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def second_cycle(bucket, foldername,comm_of_comm):\n",
    "    \n",
    "    sub_community_features1 = pd.DataFrame(columns = ['comm_group', 'Customer', 'Agent', 'Merchant', 'total_amount',\n",
    "           'mpesa_fee_revenue', 'mpesa_comm_expense', 'number_of_nodes',\n",
    "           'number_of_transactions', 'Business','Super Agent','Unknown','ave_trans_amount', 'revenue_margin'])\n",
    "\n",
    "    for i in comm_of_comm['comm_group']:\n",
    "\n",
    "\n",
    "        foldername2=f'{foldername}_group_{i}'      \n",
    "        group_data = wr.s3.read_csv(f's3://aws-glue-assets-461675780654-af-south-1/output_data/mpesa/tanz/test/tanzania_comm_abuse_output/groups/{foldername}/sub_comm_rollup{i}.csv')\n",
    "\n",
    "        foldername3=f'submodels/{foldername}/{foldername2}'\n",
    "        print(foldername2)\n",
    "        main_graph = graph_model_saving(group_data, foldername3, bucket)  #artefact\n",
    "        comm_dict, communities =  community_detection(main_graph)  #show outputs \n",
    "\n",
    "        prefix = f'groups/subgroups/{foldername}/{foldername2}'\n",
    "        sub_community_features= sub_feature_engineering(communities, group_data, prefix,comm_dict,i)  #show output #includes group artefact\n",
    "        sub_community_features = sub_community_features[(sub_community_features['revenue_margin'] <0) &  (sub_community_features['number_of_transactions'] >10) ]\n",
    "\n",
    "\n",
    "        if sub_community_features.shape[0] != 0:\n",
    "         \n",
    "            sub_community_features['comm_group'] = sub_community_features.apply(lambda row: f'{int(i)}_{int(row.comm_group)}', axis=1)\n",
    "            sub_community_features1 = pd.concat([sub_community_features1, sub_community_features])\n",
    "\n",
    "    return sub_community_features1\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5619298f-a6a0-4967-a56d-dfa9b51c457c",
   "metadata": {},
   "source": [
    "## Second Cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7768ef64-76de-466b-b7b8-d878eee799cd",
   "metadata": {},
   "source": [
    "## Combining Files (Cycle1 + Cycle 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709c0f7c-9e4a-4914-8096-634e3520b640",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7c61364e-0ba6-49b7-a3de-e6a3feec84bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def combine_feature_files_s3(comm_of_comm, community_features1,sub_community_features, bucket, foldername):\n",
    "    \n",
    "    #Get relevant communitites from the main group features and sub group features\n",
    "    sub_community_features = sub_community_features[(sub_community_features['revenue_margin'] <0)]\n",
    "    community_features2 = community_features1[community_features1['revenue_margin'] <0]\n",
    "\n",
    "    #final feature file\n",
    "    final_features = pd.concat([community_features2, sub_community_features])\n",
    "    final_features = final_features.sort_values(by=['revenue_margin'], ascending=True)\n",
    "    final_features[['Customer','Agent','Merchant','Business','Super Agent','Unknown']] =(final_features.loc[:,['Customer','Agent','Merchant','Business','Super Agent','Unknown']].fillna(0))\n",
    "    final_features[['Customer','Agent','Merchant','Business','Super Agent','Unknown', 'number_of_nodes','number_of_transactions']] =(final_features.loc[:,['Customer','Agent','Merchant','Business',\n",
    "                                                                                                                'Super Agent','Unknown','number_of_nodes','number_of_transactions']]).astype(int)\n",
    "    df = final_features[['comm_group', 'Customer', 'Agent','Super Agent', 'Merchant', 'Business', 'Unknown',\n",
    "                                     'total_amount', 'mpesa_fee_revenue', 'mpesa_comm_expense',\n",
    "                                     'number_of_nodes', 'number_of_transactions', \n",
    "                                     'ave_trans_amount', 'revenue_margin']]\n",
    "    df['run_date'] = datetime.today().date()\n",
    "    features_dest = f'output_data/mpesa/tanz/test/tanzania_comm_abuse_output/features/{foldername}'\n",
    "    save_dataframe(df, bucket, features_dest, 'combined_comm_feat') #artefact\n",
    "\n",
    "    #4CIT\n",
    "    path = f'output_data/mpesa/tanz/test/tanzania_comm_abuse_output/4CIT/{foldername}'\n",
    "    # path = f'output_data/mpesa/Tanzania/usecases/commissionAbuse/{foldername}'\n",
    "    save_dataframe(df, bucket, path, 'comm_feat') #artefact\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda26992-25f0-4da1-9651-dd524eb10395",
   "metadata": {},
   "source": [
    "### Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "001145a4-d3ba-4302-b161-f1e860118c93",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17 µs, sys: 0 ns, total: 17 µs\n",
      "Wall time: 26.9 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def combine_group_files_s3(foldername, bucket,final_features):\n",
    "    \n",
    "    #main groups\n",
    "    path2 = f'output_data/mpesa/tanz/test/tanzania_comm_abuse_output/groups/{foldername}'\n",
    "    main_group = get_data(bucket, path2, 'CSV') #show output \n",
    "\n",
    "\n",
    "    #sub groups\n",
    "    path3 = f'output_data/mpesa/tanz/test/tanzania_comm_abuse_output/groups/subgroups/{foldername}/'\n",
    "    sub_main_group = get_data(bucket, path3, 'CSV')\n",
    " \n",
    "\n",
    "    #combine main and sub to create final group file\n",
    "    combined_groups = pd.concat([main_group, sub_main_group])\n",
    "    #only filter on relevant communities of interest - only the ones in the features file\n",
    "    final_groups = combined_groups[(combined_groups['comm_group']).isin(final_features['comm_group'].unique())]\n",
    "    final_groups['run_date'] = datetime.today().date()\n",
    "\n",
    "    path = f'output_data/mpesa/tanz/test/tanzania_comm_abuse_output/groups/{foldername}'\n",
    "    save_dataframe(final_groups, bucket, path, 'combined_comm_groups') #artefact\n",
    "\n",
    "    #4CIT\n",
    "    path = f'output_data/mpesa/tanz/test/tanzania_comm_abuse_output/4CIT/{foldername}'\n",
    "    # path = f'output_data/mpesa/Tanzania/usecases/commissionAbuse/{foldername}'\n",
    "    save_dataframe(final_groups, bucket, path, 'comm_groups') #artefact\n",
    "    \n",
    "    return main_group, sub_main_group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649c3caa-d744-41c4-b24f-9e9c9b6af496",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c06a029-ed01-4a52-8f96-9d13e29a7d76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    bucket = 'aws-glue-assets-461675780654-af-south-1'\n",
    "    foldername = get_file_name()\n",
    "    prefix = f'output_data/mpesa/tanz/test/tanz_commission_abuse_input/{foldername}'\n",
    "    filetype = 'PARQUET'\n",
    "\n",
    "    \n",
    "    comm_of_comm,community_features = first_cycle(bucket,prefix,filetype,foldername)\n",
    "    \n",
    "    sub_community_features = second_cycle(bucket, foldername,comm_of_comm)\n",
    "    \n",
    "    final_features = combine_feature_files_s3(comm_of_comm, sub_community_features, community_features, bucket, foldername)\n",
    "    \n",
    "    main_group,sub_main_group = combine_group_files_s3(foldername, bucket,final_features)\n",
    "    \n",
    "#     visual_main = main_group[(main_group['comm_group']).isin( final_features['comm_group'].unique())]\n",
    "#     write_graphs_to_s3(visual_main,foldername,'main')\n",
    "    \n",
    "#     visual_sub = sub_main_group[(sub_main_group['comm_group']).isin( final_features['comm_group'].unique())]\n",
    "#     write_graphs_to_s3(visual_sub,foldername, 'sub')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6604cc07-132c-4343-9e32-044670b49305",
   "metadata": {},
   "source": [
    "## Testing individual step separately - Ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47812a9b-9305-4224-a74d-03df5bd48635",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "bucket = 'aws-glue-assets-461675780654-af-south-1'\n",
    "foldername = get_file_name()\n",
    "prefix = f'output_data/mpesa/tanz/test/tanz_commission_abuse_input/{foldername}'\n",
    "filetype = 'PARQUET'\n",
    "\n",
    "\n",
    "comm_of_comm,community_features = first_cycle(bucket,prefix,filetype,foldername)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "131a3b6e-3c48-4355-a336-afd912bab12c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7 µs, sys: 1e+03 ns, total: 8 µs\n",
      "Wall time: 16.5 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sub_community_features = second_cycle(bucket, foldername,comm_of_comm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "04e3bd89-e8d9-4676-be16-3cfa89e3b573",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30.5 ms, sys: 7.98 ms, total: 38.5 ms\n",
      "Wall time: 170 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "final_features = combine_feature_files_s3(comm_of_comm, sub_community_features, community_features, bucket, foldername)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f5e9afee-0114-454e-8ab5-e4f1b67d7714",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "main_group,sub_main_group = combine_group_files_s3(foldername, bucket,final_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "89d3b461-55b0-4143-90ce-7a7e4654f8df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# import os\n",
    "# visual_main = main_group[(main_group['comm_group']).isin( final_features['comm_group'].unique())]\n",
    "# write_graphs_to_s3(visual_main,foldername,'main')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08114740-f117-4d73-9058-221f16efc4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# visual_sub = sub_main_group[(sub_main_group['comm_group']).isin( final_features['comm_group'].unique())]\n",
    "# write_graphs_to_s3(visual_sub,foldername, 'sub')"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.r5.24xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Base Python 2.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:af-south-1:559312083959:image/sagemaker-base-python-38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
